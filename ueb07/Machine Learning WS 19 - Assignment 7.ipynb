{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adrian Gruszczynski / Yann Salimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode:\n",
    "    def __init__(self, feature, threshold):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def predict(self, x):\n",
    "        if not self._is_initialized:\n",
    "            raise ValueError('node is not initialized')\n",
    "        if x[self.feature] < self.threshold:\n",
    "            return self.left.predict(x)\n",
    "        else:\n",
    "            return self.right.predict(x)\n",
    "\n",
    "    @property\n",
    "    def _is_initialized(self):\n",
    "        return self.left and self.right\n",
    "\n",
    "\n",
    "class DTLeaf:\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, _):\n",
    "        return self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self):\n",
    "        self.classifiers = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def fit(self, X, y, classifiers, M):\n",
    "        n_classifiers = len(classifiers)\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        self.classifiers = []\n",
    "        self.alpha = []\n",
    "\n",
    "        # initialize weights\n",
    "        weights = np.ones(n_samples)\n",
    "\n",
    "        # initialize scouting matrix\n",
    "        S = np.empty((n_samples, n_classifiers))\n",
    "        for k, classifier in enumerate(classifiers):\n",
    "            S[:, k] = [2 * (classifier.predict(X[i]) == y[i]) - 1\n",
    "                       for i in range(n_samples)]\n",
    "\n",
    "        for m in range(M):\n",
    "            # select classifier that minimizes W_e\n",
    "            best_W_e = np.Inf\n",
    "            classifier_data = None\n",
    "            for k, classifier in enumerate(classifiers):\n",
    "                W_e = np.sum(weights[np.where(S[:, k] == -1)])\n",
    "\n",
    "                if W_e < best_W_e:\n",
    "                    best_W_e = W_e\n",
    "                    classifier_data = k, classifier\n",
    "\n",
    "            # calculate alpha of the selected classifier\n",
    "            W = np.sum(weights)\n",
    "            e_m = best_W_e / W\n",
    "            alpha = np.log((1 - e_m) / e_m) / 2\n",
    "\n",
    "            # update weights\n",
    "            k, classifier = classifier_data\n",
    "            weights = weights * np.exp(-S[:, k] * alpha)\n",
    "\n",
    "            self.classifiers.append(classifier)\n",
    "            self.alpha.append(alpha)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError('AdaBoost is not fitted')\n",
    "        score = 0\n",
    "        for classifier, alpha in zip(self.classifiers, self.alpha):\n",
    "            y_pred = classifier.predict(x)\n",
    "            score += (2 * y_pred - 1) * alpha\n",
    "        y_pred = 0 if score <= 0 else 1\n",
    "        return y_pred\n",
    "\n",
    "    @property\n",
    "    def _is_fitted(self):\n",
    "        return self.classifiers and self.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    probabilities = np.bincount(X) / len(X)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "\n",
    "def buildDT(X, y, n_features_sampled=None, max_depth=None):\n",
    "    best_information_gain, node_data = 0, None\n",
    "    n_samples, n_features = X.shape\n",
    "    H_before_split = entropy(y)\n",
    "\n",
    "    if n_features_sampled:\n",
    "        features = np.random.choice(n_features,\n",
    "                                    min(n_features, n_features_sampled),\n",
    "                                    replace=False)\n",
    "    else:\n",
    "        features = np.arange(n_features)\n",
    "\n",
    "    for feature in features:\n",
    "        X_feature = X[:, feature]\n",
    "        threshold = np.mean(X_feature)\n",
    "\n",
    "        left_idx = X_feature < threshold\n",
    "        right_idx = X_feature >= threshold\n",
    "\n",
    "        y_left = y[left_idx]\n",
    "        y_right = y[right_idx]\n",
    "        p_y_left = len(y_left) / n_samples\n",
    "        p_y_right = len(y_right) / n_samples\n",
    "\n",
    "        H_after_split = p_y_left * entropy(y_left) + p_y_right * entropy(y_right)\n",
    "        information_gain = H_before_split - H_after_split\n",
    "\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            node_data = feature, threshold, left_idx, y_left, right_idx, y_right\n",
    "\n",
    "    if max_depth == 0 or not best_information_gain:\n",
    "        most_frequent_y = np.argmax(np.bincount(y))\n",
    "        return DTLeaf(most_frequent_y)\n",
    "    else:\n",
    "        feature, threshold, left_idx, y_left, right_idx, y_right = node_data\n",
    "        depth = None if max_depth is None else max_depth - 1\n",
    "        node = DTNode(feature, threshold)\n",
    "        node.left = buildDT(X[left_idx], y_left, n_features_sampled, depth)\n",
    "        node.right = buildDT(X[right_idx], y_right, n_features_sampled, depth)\n",
    "        return node\n",
    "\n",
    "\n",
    "def unison_shuffle(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('array lengths do not match')\n",
    "    idx = np.random.permutation(len(a))\n",
    "    return a[idx], b[idx]\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError('array shapes do not match')\n",
    "    return np.sum(np.equal(y_true, y_pred)) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For stumps with depth=1\n",
      "val accuracy: 82.75%\n",
      "train accuracy: 81.78%\n",
      "For stumps with depth=2\n",
      "val accuracy: 88.44%\n",
      "train accuracy: 88.78%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(12345)\n",
    "    df = np.array(pd.read_csv('spambase.data', header=None))\n",
    "\n",
    "    X, y = df[:, :-1], df[:, -1].astype(np.bool_)\n",
    "    X, y = unison_shuffle(X, y)\n",
    "\n",
    "    split = len(X) // 2\n",
    "\n",
    "    X_train, y_train = X[:split], y[:split]\n",
    "    X_val, y_val = X[split:], y[split:]\n",
    "\n",
    "    _, n_features = X.shape\n",
    "    n_features_sampled = int(np.sqrt(n_features))\n",
    "    forest_size = 50\n",
    "    decision_trees = []\n",
    "    \n",
    "    print(\"For stumps with depth=1\")\n",
    "\n",
    "    for tree_idx in range(forest_size):\n",
    "        sampled_idx = np.random.randint(0, high=split, size=split)\n",
    "        X_bootstrap, y_bootstrap = X_train[sampled_idx], y_train[sampled_idx]\n",
    "        decision_tree = buildDT(X_bootstrap, y_bootstrap,\n",
    "                                n_features_sampled=n_features_sampled,\n",
    "                                max_depth=1)    # 1 split\n",
    "        decision_trees.append(decision_tree)\n",
    "\n",
    "    M = 50\n",
    "    decision_forest = AdaBoost()\n",
    "    decision_forest.fit(X_train, y_train, decision_trees, M)\n",
    "\n",
    "    y_val_pred = np.empty(y_val.shape)\n",
    "    for i in range(len(X_val)):\n",
    "        y_val_pred[i] = decision_forest.predict(X_val[i])\n",
    "\n",
    "    val_accuracy = 100 * accuracy_score(y_val, y_val_pred)\n",
    "    print('val accuracy: %.2f%%' % val_accuracy)\n",
    "\n",
    "    y_train_pred = np.empty(y_train.shape)\n",
    "    for i in range(len(X_train)):\n",
    "        y_train_pred[i] = decision_forest.predict(X_train[i])\n",
    "\n",
    "    train_accuracy = 100 * accuracy_score(y_train, y_train_pred)\n",
    "    print('train accuracy: %.2f%%' % train_accuracy)\n",
    "    \n",
    "    print(\"For stumps with depth=2\")\n",
    "\n",
    "    for tree_idx in range(forest_size):\n",
    "        sampled_idx = np.random.randint(0, high=split, size=split)\n",
    "        X_bootstrap, y_bootstrap = X_train[sampled_idx], y_train[sampled_idx]\n",
    "        decision_tree = buildDT(X_bootstrap, y_bootstrap,\n",
    "                                n_features_sampled=n_features_sampled,\n",
    "                                max_depth=2) # 2 splits\n",
    "        decision_trees.append(decision_tree)\n",
    "\n",
    "    M = 50\n",
    "    decision_forest = AdaBoost()\n",
    "    decision_forest.fit(X_train, y_train, decision_trees, M)\n",
    "\n",
    "    y_val_pred = np.empty(y_val.shape)\n",
    "    for i in range(len(X_val)):\n",
    "        y_val_pred[i] = decision_forest.predict(X_val[i])\n",
    "\n",
    "    val_accuracy = 100 * accuracy_score(y_val, y_val_pred)\n",
    "    print('val accuracy: %.2f%%' % val_accuracy)\n",
    "\n",
    "    y_train_pred = np.empty(y_train.shape)\n",
    "    for i in range(len(X_train)):\n",
    "        y_train_pred[i] = decision_forest.predict(X_train[i])\n",
    "\n",
    "    train_accuracy = 100 * accuracy_score(y_train, y_train_pred)\n",
    "    print('train accuracy: %.2f%%' % train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost performs better when using stronger weak learners. The reason for that is, that a commitee consisting of stronger experts can predict even better than commitee of weaker experts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
