{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adrian Gruszczynski / Yann Salimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Least-Squares Linear Regression\n",
    "Using the closed-form expression from the lecture, implement Linear Regression in Python\n",
    "(incl. Numpy, Pandas, Matplotlib) on a Jupyter Notebook. Train on the training set of the\n",
    "\"ZIP code\"-Dataset and test on its test set.\n",
    "\n",
    "$$\\hat{\\boldsymbol\\beta} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.coefficients = None\n",
    "    \n",
    "    def fit(self, train_X, train_y):\n",
    "        a = np.linalg.inv(np.dot(train_X.T, train_X))\n",
    "        b = np.dot(train_X.T, train_y)\n",
    "        self.coefficients = np.dot(a,b)\n",
    "    \n",
    "    def predict(self, _test_X):\n",
    "        return np.dot(_test_X, self.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_training_data = np.array(pd.read_csv('zip.train', sep=' ', header=None),\n",
    "                          dtype=np.float32)\n",
    "_test_data = np.array(pd.read_csv('zip.test', sep=' ', header=None),\n",
    "                      dtype=np.float32)\n",
    "_train_x = _training_data[:, 1:-1]\n",
    "_train_X = np.hstack((np.ones((_train_x.shape[0],1)), _train_x))\n",
    "_train_y = _training_data[:, 0].astype(np.uint8)\n",
    "_test_x = _test_data[:, 1:]\n",
    "_test_X = np.hstack((np.ones((_test_x.shape[0],1)), _test_x))\n",
    "_test_y = _test_data[:, 0].astype(np.uint8)\n",
    "_zip_classifier = LinearRegression()\n",
    "_zip_classifier.fit(_train_X, _train_y)\n",
    "_y_pred = np.round(_zip_classifier.predict(_test_X), decimals=0).astype(np.int8)\n",
    "_conf_m = pd.crosstab(pd.Series(_test_y, name='Actual'),\n",
    "pd.Series(_y_pred, name='Predicted'))\n",
    "_accuracy = np.sum(np.equal(_y_pred,_test_y)) / len(_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the Confusion Matrix and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.22521175884404585\n",
      "Predicted  -3   -2   -1    0    1    2    3    4    5    6    7    8    9   \\\n",
      "Actual                                                                       \n",
      "0            2   11   44   71  108   65   39   12    4    3    0    0    0   \n",
      "1            0    0    0    6  119   84   35   14    3    3    0    0    0   \n",
      "2            0    1    3   10   19   42   45   45   23    7    3    0    0   \n",
      "3            0    0    1    1   10   24   38   42   36   12    2    0    0   \n",
      "4            0    0    0    0    1    5   11   41   46   41   38   14    3   \n",
      "5            1    0    1    1   10   17   23   33   31   28   11    3    1   \n",
      "6            0    0    0    0    5    8   16   46   54   35    4    2    0   \n",
      "7            0    0    0    0    0    1    3   10   20   45   47   18    3   \n",
      "8            0    0    1    1    1    3    4   24   30   50   34   14    4   \n",
      "9            0    0    0    0    0    0    2    9   17   22   55   56   14   \n",
      "\n",
      "Predicted   10  \n",
      "Actual          \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n",
      "5            0  \n",
      "6            0  \n",
      "7            0  \n",
      "8            0  \n",
      "9            2  \n"
     ]
    }
   ],
   "source": [
    "print('Accuracy ', _accuracy)\n",
    "print(_conf_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a good way of encoding the labels?\n",
    "Predicted labels are continous values and so they cannot be compared with the actual test values. To overcome the problem, the predicted values could be rounded and encoded as type int8.\n",
    "Alternatively, one could encode the training labels as linearly independent vectors in $\\mathbb{R}^{10}$ and choose the best fit to determine the most likely category. An exemplary calculation using that approach is attached below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the problem with using Linear Regression for Classification?\n",
    "Linear regression predicts continous values whereas classification problems are of a discrete\n",
    "nature. It is advised to use logistic regression for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Encoding labels as (standard basis) vectors in $\\mathbb{R}^{10}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base(j):\n",
    "    return np.eye(1, 10, j)\n",
    "\n",
    "def C(correct, predicted):\n",
    "    assert len(correct) == len(predicted)\n",
    "    c = np.zeros((10, 10), dtype=int)\n",
    "    for i, j in zip(correct, predicted):\n",
    "        c[i, j] += 1\n",
    "    return c\n",
    "\n",
    "def ratio(correct, predicted):\n",
    "    return np.trace(C(correct, predicted))/len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('zip.train')\n",
    "y, X = train.T[0].astype(int), train.T[1:].T\n",
    "y_v = np.array([base(j) for j in y]).reshape(len(X), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pseudo-inverse $(\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T}$ = np.linalg.pinv(X), for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.dot(np.linalg.pinv(X), y_v)\n",
    "y_mle = np.dot(X, beta)\n",
    "y_mle = [np.argmax(a) for a in y_mle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1177,    0,    1,    4,    3,    1,    7,    0,    1,    0],\n",
       "       [   0, 1002,    1,    0,    0,    0,    0,    0,    1,    1],\n",
       "       [   9,    5,  648,   19,   15,    2,   13,    6,   12,    2],\n",
       "       [   8,    0,    6,  611,    0,    9,    0,    9,   13,    2],\n",
       "       [   2,   25,    7,    0,  589,    3,    5,    1,    4,   16],\n",
       "       [  17,    1,    3,   31,   10,  477,    9,    0,    5,    3],\n",
       "       [  10,    7,    9,    1,   13,    9,  608,    0,    7,    0],\n",
       "       [   4,    4,    1,    0,    4,    1,    0,  593,    3,   35],\n",
       "       [  24,    6,    4,   17,   14,   11,    5,    3,  452,    6],\n",
       "       [   1,    7,    0,    1,   26,    6,    0,   20,    5,  578]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C(y, y_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9237415992319298"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio(y, y_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.loadtxt('zip.test')\n",
    "y, X = test.T[0].astype(int), test.T[1:].T\n",
    "y_v = np.array([base(j) for j in y]).reshape(len(X), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mle = np.dot(X, beta)\n",
    "y_mle = [np.argmax(a) for a in y_mle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[347,   2,   0,   1,   3,   1,   4,   0,   0,   1],\n",
       "       [  0, 254,   0,   2,   3,   0,   3,   0,   1,   1],\n",
       "       [  7,   3, 159,   8,  10,   1,   1,   1,   8,   0],\n",
       "       [  5,   0,   4, 137,   2,   9,   0,   2,   3,   4],\n",
       "       [  3,   7,   4,   0, 169,   1,   3,   2,   1,  10],\n",
       "       [  9,   1,   0,  20,   2, 122,   0,   1,   1,   4],\n",
       "       [  3,   2,   4,   0,   5,   4, 151,   0,   1,   0],\n",
       "       [  3,   1,   1,   1,   7,   0,   0, 130,   0,   4],\n",
       "       [  8,   3,   3,  15,   3,   8,   1,   2, 119,   4],\n",
       "       [  0,   3,   0,   0,   7,   0,   0,   7,   2, 158]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C(y, y_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy for test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8699551569506726"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio(y, y_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why use quadratic loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    "\n",
    "$$y_i = \\hat{y}(x_i; w) + \\epsilon_i$$\n",
    "\n",
    "With $\\epsilon_i \\sim \\mathcal{N}(0, 1) = \\frac{1}{2\\pi}exp(\\frac{-\\epsilon_0,i^2}{2})$ and some arbitrary constant factor $\\sigma$ we can write\n",
    "\n",
    "$$y_i = \\hat{y}(x_i;w) + \\sigma\\epsilon_i$$\n",
    "\n",
    "It follows that for linear $\\hat{y}(x_i;w)$, $y_i$ also normally distributed with\n",
    "\n",
    "$$y_i \\sim \\mathcal{N}(\\hat{y}(x_i;w), \\sigma)$$\n",
    "\n",
    "We therefore can write for the likelihood\n",
    "\n",
    "$$L(X,Y,w)=\\prod_{i=1}^n p(y_i=\\hat{y}(x_i;w)) \\sim\\prod_{i=1}^n exp(\\frac{-1}{2\\sigma^2}(y_i - \\hat{y}(x_i;w))^2) = exp(\\frac{-1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\hat{y}(x_i;w))^2)$$\n",
    "\n",
    "We are interested $\\hat{w}$ that maximizes the likelihood, or the logarithm of the likelihood\n",
    "\n",
    "$$\\hat{w} = \\text{argmax}_w L(X,Y,w) = \\text{argmax}_w \\log L(X,Y,w),$$\n",
    "\n",
    "which is equivalent because $\\log L(X,Y,w)$ strictly monotone. It follows from above that with positive constant $\\alpha$\n",
    " \n",
    "$$\\log L(X,Y,w) = \\frac{-1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\hat{y}(x_i;w))^2 + \\alpha.$$\n",
    "\n",
    "Since the first term in the sum is negative, we can immediately see that the whole term is maximized by\n",
    "\n",
    "$$\\text{argmax}_w \\log L(X,Y,w) = \\text{argmax}_w \\frac{-1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\hat{y}(x_i;w))^2 = \\text{argmin}_w \\sum_{i=1}^n(y_i - \\hat{y}(x_i;w))^2$$\n",
    "\n",
    "It follows that\n",
    "$$\\hat{w} = \\text{argmax}_w L(X,Y,w) = \\text{argmin}_w \\sum_{i=1}^n(y_i - \\hat{y}(x_i;w))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_metadata": {
   "author": "Adrian Gruszczynski / Yann Salimi",
   "title": "Mustererkennung/Machine Learning WiSe 18/19 - Assignment 2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
