{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adrian Gruszczynski / Yann Salimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode:\n",
    "    def __init__(self, feature, threshold):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def predict(self, x):\n",
    "        if not self._is_initialized:\n",
    "            raise ValueError('node is not initialized')\n",
    "        if x[self.feature] < self.threshold:\n",
    "            return self.left.predict(x)\n",
    "        else:\n",
    "            return self.right.predict(x)\n",
    "\n",
    "    @property\n",
    "    def _is_initialized(self):\n",
    "        return self.left and self.right\n",
    "\n",
    "\n",
    "class DTLeaf:\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "\n",
    "    def predict(self, _):\n",
    "        return self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X):\n",
    "    probabilities = np.bincount(X) / len(X)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "\n",
    "def buildDT(X, y):\n",
    "    best_information_gain, node_data = 0, None\n",
    "    n_samples, n_features = X.shape\n",
    "    H_before_split = entropy(y)\n",
    "\n",
    "    for feature in range(n_features):\n",
    "        X_feature = X[:, feature]\n",
    "        threshold = np.mean(X_feature)\n",
    "        left_idx = X_feature < threshold\n",
    "        right_idx = X_feature >= threshold\n",
    "        y_left = y[left_idx]\n",
    "        y_right = y[right_idx]\n",
    "        p_y_left = len(y_left) / n_samples\n",
    "        p_y_right = len(y_right) / n_samples\n",
    "        H_after_split = p_y_left * entropy(y_left) + p_y_right * entropy(y_right)\n",
    "        information_gain = H_before_split - H_after_split\n",
    "\n",
    "        if information_gain > best_information_gain:\n",
    "            best_information_gain = information_gain\n",
    "            node_data = feature, threshold, left_idx, y_left, right_idx, y_right\n",
    "\n",
    "    if not best_information_gain:\n",
    "        return DTLeaf(y[0])\n",
    "    else:\n",
    "        feature, threshold, left_idx, y_left, right_idx, y_right = node_data\n",
    "        node = DTNode(feature, threshold)\n",
    "        node.left = buildDT(X[left_idx], y_left)\n",
    "        node.right = buildDT(X[right_idx], y_right)\n",
    "        return node\n",
    "\n",
    "\n",
    "def unison_shuffle(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError('array lengths do not match')\n",
    "    idx = np.random.permutation(len(a))\n",
    "    return a[idx], b[idx]\n",
    "\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError('array shapes do not match')\n",
    "    return np.sum(np.equal(y_true, y_pred)) / len(y_true)\n",
    "\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    y_actual = pd.Series(y_true, name='Actual')\n",
    "    y_pred = pd.Series(y_pred, name='Predicted')\n",
    "    return pd.crosstab(y_actual, y_pred,\n",
    "                       rownames=['Actual'],\n",
    "                       colnames=['Predicted'],\n",
    "                       margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "df = np.array(pd.read_csv('spambase.data', header=None))\n",
    "\n",
    "X, y = df[:, :-1], df[:, -1].astype(np.bool_)\n",
    "X, y = unison_shuffle(X, y)\n",
    "\n",
    "split = len(X) // 2\n",
    "\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_val, y_val = X[split:], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = buildDT(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   0.0  1.0   All\n",
      "Actual                    \n",
      "False       849  533  1382\n",
      "True        574  344   918\n",
      "All        1423  877  2300\n",
      "decision tree accuracy: 90.53%\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.empty(y_val.shape)\n",
    "for i in range(len(X_val)):\n",
    "    y_pred[i] = decision_tree.predict(X_val[i])\n",
    "\n",
    "conf_m_tree = confusion_matrix(y_train, y_pred)\n",
    "accuracy = 100 * accuracy_score(y_val, y_pred)\n",
    "print(conf_m_tree)\n",
    "print('decision tree accuracy: %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assume that classifying a genuine E-Mail as spam is ten times worse than classifying spam as genuine. How would you change the design of your decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fullfill the assumption's requirement pruning would be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions shall be only positive if the certainty, that a given sample is spam is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this a vote mechanism could be implemented, that classifies a sample positive only if 90% of labels are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Can Information Gain be negative? Try to prove your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the decision trees the information gain is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$IG(Y|X) = H(Y) - H(Y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas $H(Y)$ is the entropy of the parent node and $H(Y|X)$ the entropy of the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume worst case scenario in which node $Y$ cannnot be splitted anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the enrtopy $H(Y)$ equals this of $H(Y|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It holds that $H(Y) = H(Y|X) - H(Y|X) >=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formally "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Gibb's inequality it follows that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_{KL}(P||Q)=\\sum_{i=1}^{n}p_i\\log_2\\frac{p_i}{q_i} \\geq 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only case $D_{KL} = 0$ is when $P=Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the information gain can not be negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTForest:\n",
    "    def __init__(self, decision_trees):\n",
    "        self.decision_trees = decision_trees\n",
    "\n",
    "    def predict(self, x):\n",
    "        y_pred = [dt.predict(x) for dt in self.decision_trees]\n",
    "        most_frequent_y = np.argmax(np.bincount(y_pred))\n",
    "        return most_frequent_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_size = 5\n",
    "decision_trees = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tree_idx in range(forest_size):\n",
    "    sampled_idx = np.random.randint(0, high=split, size=split)\n",
    "    X_bootstrap, y_bootstrap = X_train[sampled_idx], y_train[sampled_idx]\n",
    "    decision_tree = buildDT(X_bootstrap, y_bootstrap)\n",
    "    decision_trees.append(decision_tree)\n",
    "\n",
    "decision_forest = DTForest(decision_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.empty(y_val.shape)\n",
    "for i in range(len(X_val)):\n",
    "    y_pred[i] = decision_forest.predict(X_val[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   0.0  1.0   All\n",
      "Actual                    \n",
      "False       848  534  1382\n",
      "True        583  335   918\n",
      "All        1431  869  2300\n",
      "forest accuracy: 93.83%\n"
     ]
    }
   ],
   "source": [
    "conf_m_tree = confusion_matrix(y_train, y_pred)\n",
    "accuracy = 100 * accuracy_score(y_val, y_pred)\n",
    "print(conf_m_tree)\n",
    "print('forest accuracy: %.2f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a good number of trees in the forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge increase in number of trees in the forest does not boost the accuracy appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the difference in accuracy between a forest with 5 and a forest with 100 trees equals 0.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the best way to decide?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to decide would be to consider the probability that the given sample is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability higher than a certain threshold could be classfied as positive otherwise as negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
